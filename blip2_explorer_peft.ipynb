{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ab9e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24160a692574e33bdc2e9bd02d35420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `BlipImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configure 8-bit quantization to reduce memory usage\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load model with quantization and automatic device mapping\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20467a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing data/uic...\n",
      "Successfully extracted uic.zip to data/uic\n",
      "Loaded 3176 images with captions\n",
      "Example entry:\n",
      "  Image: uic_img_1.jpg\n",
      "  Number of captions: 5\n",
      "  First caption: A dark brown turtle paddles through the water with its limbs .\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "zip_file_path = \"uic.zip\"\n",
    "extract_to_path = \"data/uic\"\n",
    "dataset_base = \"data/uic/UIC(underwater image captioning dataset)\"\n",
    "image_dir = os.path.join(dataset_base, \"uic_224x224_image\")\n",
    "captions_file = os.path.join(dataset_base, \"UIC-captions.txt\")\n",
    "\n",
    "# Delete existing data/uic directory if it exists\n",
    "if os.path.exists(extract_to_path):\n",
    "    print(f\"Removing existing {extract_to_path}...\")\n",
    "    shutil.rmtree(extract_to_path)\n",
    "\n",
    "# Extract zip file if it exists\n",
    "if os.path.exists(zip_file_path):\n",
    "    os.makedirs(extract_to_path, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to_path)\n",
    "    print(f\"Successfully extracted {zip_file_path} to {extract_to_path}\")\n",
    "\n",
    "# Load captions from UIC-captions.txt\n",
    "def load_captions(captions_path):\n",
    "    \"\"\"Parse UIC captions file and return a dictionary mapping image filenames to their captions.\"\"\"\n",
    "    image_captions = {}\n",
    "    \n",
    "    with open(captions_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                # Split on first space to separate image#id from caption\n",
    "                parts = line.split(' ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    img_id, caption = parts\n",
    "                    # Extract image filename (before the #)\n",
    "                    img_filename = img_id.split('#')[0]\n",
    "                    \n",
    "                    # Store captions in a list for each image\n",
    "                    if img_filename not in image_captions:\n",
    "                        image_captions[img_filename] = []\n",
    "                    image_captions[img_filename].append(caption)\n",
    "    \n",
    "    return image_captions\n",
    "\n",
    "# Load the dataset\n",
    "captions_dict = load_captions(captions_file)\n",
    "image_paths = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
    "\n",
    "# Create dataset as list of (image_path, captions) tuples\n",
    "dataset = []\n",
    "for img_filename in image_paths:\n",
    "    img_path = os.path.join(image_dir, img_filename)\n",
    "    captions = captions_dict.get(img_filename, [])\n",
    "    if captions:  # Only include images that have captions\n",
    "        dataset.append({\n",
    "            'image_path': img_path,\n",
    "            'image_filename': img_filename,\n",
    "            'captions': captions\n",
    "        })\n",
    "\n",
    "print(f\"Loaded {len(dataset)} images with captions\")\n",
    "print(f\"Example entry:\")\n",
    "print(f\"  Image: {dataset[0]['image_filename']}\")\n",
    "print(f\"  Number of captions: {len(dataset[0]['captions'])}\")\n",
    "print(f\"  First caption: {dataset[0]['captions'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424447a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2858\n",
      "Validation samples: 318\n",
      "trainable params: 5,242,880 || all params: 3,750,004,736 || trainable%: 0.1398\n",
      "PEFT model on device: cuda:0\n",
      "Starting PEFT fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kt/miniconda3/envs/blip2_explorer/lib/python3.10/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/kt/miniconda3/envs/blip2_explorer/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/home/kt/miniconda3/envs/blip2_explorer/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 12/537 01:22 < 1:12:19, 0.12 it/s, Epoch 0.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Create PyTorch Dataset class\n",
    "class UICDataset(Dataset):\n",
    "    def __init__(self, data, processor):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = Image.open(item['image_path']).convert('RGB')\n",
    "        # Use the first caption for training\n",
    "        caption = item['captions'][0]\n",
    "        \n",
    "        # Process image and text\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        # Set labels for language modeling\n",
    "        encoding[\"labels\"] = encoding[\"input_ids\"].clone()\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "# Split dataset into train and validation (90/10 split)\n",
    "random.seed(42)\n",
    "random.shuffle(dataset)\n",
    "split_idx = int(0.9 * len(dataset))\n",
    "train_dataset = dataset[:split_idx]\n",
    "eval_dataset = dataset[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Create dataset objects\n",
    "train_data = UICDataset(train_dataset, processor)\n",
    "eval_data = UICDataset(eval_dataset, processor)\n",
    "\n",
    "# Configure LoRA for PEFT\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # LoRA scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Apply PEFT to model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model = peft_model.to(device)\n",
    "peft_model.print_trainable_parameters()\n",
    "print(f\"PEFT model on device: {next(peft_model.parameters()).device}\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip2-uic-peft\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting PEFT fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "peft_model.save_pretrained(\"./blip2-uic-finetuned\")\n",
    "\n",
    "processor.save_pretrained(\"./blip2-uic-finetuned\")\n",
    "print(\"Fine-tuned model saved to ./blip2-uic-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15e039",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on a sample image\n",
    "from peft import PeftModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load a test image\n",
    "test_sample = eval_dataset[0]\n",
    "test_image = Image.open(test_sample['image_path']).convert('RGB')\n",
    "\n",
    "# Prepare image for inference\n",
    "inputs = processor(images=test_image, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the same device as model\n",
    "device = next(peft_model.parameters()).device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate caption\n",
    "with torch.no_grad():\n",
    "    generated_ids = peft_model.generate(**inputs, max_length=128)\n",
    "    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Test Image:\", test_sample['image_filename'])\n",
    "print(\"Generated Caption:\", generated_caption)\n",
    "print(\"\\nGround Truth Captions:\")\n",
    "for i, caption in enumerate(test_sample['captions'], 1):\n",
    "    print(f\"  {i}. {caption}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip2_explorer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
