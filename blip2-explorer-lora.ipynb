{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"accelerator":"TPU","colab":{"gpuType":"V5E1","provenance":[],"toc_visible":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"0028f03da6784705bf0873b329926699":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a91778d32db41cbac8942c8a6a83ac1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0cf9b37118fb4c33b32d84bec560b455":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d52841b63364689910bef9f6aeb7c06","placeholder":"​","style":"IPY_MODEL_cb35ced5eb9f4dbf979fef581b3120f9","value":"Fetching 2 files: 100%"}},"0d52841b63364689910bef9f6aeb7c06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f30865a130a4b4c80bf301244a2cbcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f009210877e643198210edd4cbca3f6d","max":1247,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99e49eb82a1e4ab9aa4631a215451e48","value":1247}},"1234e503db894a048ec176f60d074fad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"129158f5ccac42ceb2571868f5368cda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17cb81cd30a84c73bee54d1c479d30f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4b34393c66b47f79824a89929c80c0a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6626b82461c4fa695fdf0df20cfa8e1","value":0}},"2245df6d4be8410499d7ff4498ea7740":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c105afb5b1064ecba7d1703f756801a8","placeholder":"​","style":"IPY_MODEL_75c608754a0a41389639bf0bf9d20383","value":"Loading weights: 100%"}},"237a398cd1d6453fabe6bc8608ec3c7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29025899ae8048e1be7d6949e521cedb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3900afe77fa84689bcbe9b82e1809e7f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a91778d32db41cbac8942c8a6a83ac1","value":2}},"2dd574c268fa4d1194dff030b5ab3bfc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a101cadcdace4e6cad41b35508814d67","placeholder":"​","style":"IPY_MODEL_f92b639211374dc8b17af498f9de646b","value":" 0.00/0.00 [00:00&lt;?, ?B/s]"}},"31c7921e977c472fb66a6a4737570bbc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5245b709b10b474ca400e426f0b4e15d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e45d887bbf73462b9decf0c69657b0d9","value":0}},"359505fe47d945eea183781d7ca0d9a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36c94dc153494cbea1acdce73edd97e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3900afe77fa84689bcbe9b82e1809e7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c07228067b44addbfb8870887c9ac1e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d3b8e3b185b4594b38fd5dbbda0cb8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5245b709b10b474ca400e426f0b4e15d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"534e0e24fa6c4e4fbf95e3b62c99c409":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54e36f147aa546e5a91a1d10dbd6bce1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"603b771b12d5425da772196aad20f1fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6973867973a241908b5e1b898c8af100":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8d92f28397c44c88d3447d93d76c220","placeholder":"​","style":"IPY_MODEL_fbde23102e72480889184ec14f4dcf23","value":" 1247/1247 [01:01&lt;00:00, 30.37it/s, Materializing param=vision_model.post_layernorm.weight]"}},"69830c32888b482ca7e341da95c87ddb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b28b6b0cfd894de8aaf025ff916ead9f","IPY_MODEL_17cb81cd30a84c73bee54d1c479d30f0","IPY_MODEL_2dd574c268fa4d1194dff030b5ab3bfc"],"layout":"IPY_MODEL_7eba2ffb4cc642a39272e988c06fcb05"}},"6ea15fcebe554971b0776644ca893b8c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72d70069a58b48749bbf309f1a8787a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75c608754a0a41389639bf0bf9d20383":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79d0e012e51d4b009f7392d64bea3dfc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8281c311982e48cfb7b9a96590af1e5c","IPY_MODEL_31c7921e977c472fb66a6a4737570bbc","IPY_MODEL_867a7c5db4cf4931ae2182f90fd4237f"],"layout":"IPY_MODEL_6ea15fcebe554971b0776644ca893b8c"}},"7eba2ffb4cc642a39272e988c06fcb05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8281c311982e48cfb7b9a96590af1e5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e105527ca3224bb899853236bed18de8","placeholder":"​","style":"IPY_MODEL_237a398cd1d6453fabe6bc8608ec3c7c","value":"Download complete: "}},"83d8cd7d193247adae3bef9ba8354b6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"851ce6c5c57843ab92b25dc03923d40b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85246125ac184d199411359dc8ddb0ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"867a7c5db4cf4931ae2182f90fd4237f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea77f4fcf89f4f78aebcea720ca1917b","placeholder":"​","style":"IPY_MODEL_36c94dc153494cbea1acdce73edd97e6","value":" 0.00/0.00 [00:00&lt;?, ?B/s]"}},"86fd6f6264924d249481c33126ab1328":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8861ef754cc2443685b6aec2dacaf222":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0a827a879a540f8820d095412e8bce9","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_603b771b12d5425da772196aad20f1fb","value":2}},"8d165de70f2a46c0b69f2940197a01ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1234e503db894a048ec176f60d074fad","max":319,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d32a36560db84a87859021d5377ab618","value":319}},"908a269de4f1444581931005bf73e9a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"926ed50b87ea4347891156ccaec2504d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93db0cdb35c9469096c0217daeca9cc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a34da4fdad404413b5c5c2d787e5ced9","placeholder":"​","style":"IPY_MODEL_83d8cd7d193247adae3bef9ba8354b6c","value":" 2/2 [00:00&lt;00:00, 82.16it/s]"}},"9910ef5be6774899a16ebea52f16bccd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99e49eb82a1e4ab9aa4631a215451e48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c4a2a1e556945d5b3d0ea41c04e6de1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0cf9b37118fb4c33b32d84bec560b455","IPY_MODEL_8861ef754cc2443685b6aec2dacaf222","IPY_MODEL_93db0cdb35c9469096c0217daeca9cc2"],"layout":"IPY_MODEL_d053a51b5a434e3faae019e71736c0a5"}},"9d22e9939deb485491fea82916d7b2b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f9464bb8ac948b1a996653ce65c089f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a101cadcdace4e6cad41b35508814d67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a34da4fdad404413b5c5c2d787e5ced9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9c4a04f99c64ccc9da33c768ababa4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_534e0e24fa6c4e4fbf95e3b62c99c409","placeholder":"​","style":"IPY_MODEL_85246125ac184d199411359dc8ddb0ab","value":" 2/2 [00:00&lt;00:00, 52.20it/s]"}},"aca05b685b0b4ecf836fe2bef55eb8b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0a827a879a540f8820d095412e8bce9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b28b6b0cfd894de8aaf025ff916ead9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aca05b685b0b4ecf836fe2bef55eb8b1","placeholder":"​","style":"IPY_MODEL_4d3b8e3b185b4594b38fd5dbbda0cb8d","value":"Download complete: "}},"b6b3861ec54c4ce2a86448a91891d711":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef330dde15cd466cbe2919f8e04c5420","IPY_MODEL_29025899ae8048e1be7d6949e521cedb","IPY_MODEL_a9c4a04f99c64ccc9da33c768ababa4b"],"layout":"IPY_MODEL_3c07228067b44addbfb8870887c9ac1e"}},"b88c8648e42b4fd784e84d5c4dfb509d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_359505fe47d945eea183781d7ca0d9a0","placeholder":"​","style":"IPY_MODEL_129158f5ccac42ceb2571868f5368cda","value":"Generating captions: 100%"}},"c105afb5b1064ecba7d1703f756801a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1a3f1c56391476f86fb446cfcb8b165":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c76d5d0dcf8b4983a943f7a647c77a81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0028f03da6784705bf0873b329926699","placeholder":"​","style":"IPY_MODEL_851ce6c5c57843ab92b25dc03923d40b","value":" 1247/1247 [01:08&lt;00:00, 27.45it/s, Materializing param=vision_model.post_layernorm.weight]"}},"cb35ced5eb9f4dbf979fef581b3120f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf22f8677fa549c9bc58ae4c18c265b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b88c8648e42b4fd784e84d5c4dfb509d","IPY_MODEL_8d165de70f2a46c0b69f2940197a01ce","IPY_MODEL_ef03282e925f46e2b8b9fb270888b950"],"layout":"IPY_MODEL_908a269de4f1444581931005bf73e9a4"}},"d053a51b5a434e3faae019e71736c0a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d32a36560db84a87859021d5377ab618":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d4b34393c66b47f79824a89929c80c0a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"da3c65cadda84f6c8ebcf7a75fb3438a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2245df6d4be8410499d7ff4498ea7740","IPY_MODEL_ddf30dad42c74b90ae064376bb1ce0e2","IPY_MODEL_6973867973a241908b5e1b898c8af100"],"layout":"IPY_MODEL_54e36f147aa546e5a91a1d10dbd6bce1"}},"ddf30dad42c74b90ae064376bb1ce0e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb0518b1fe834635a29d9058cf8fb44d","max":1247,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86fd6f6264924d249481c33126ab1328","value":1247}},"e105527ca3224bb899853236bed18de8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e45d887bbf73462b9decf0c69657b0d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6626b82461c4fa695fdf0df20cfa8e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea77f4fcf89f4f78aebcea720ca1917b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb0518b1fe834635a29d9058cf8fb44d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef03282e925f46e2b8b9fb270888b950":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9910ef5be6774899a16ebea52f16bccd","placeholder":"​","style":"IPY_MODEL_9d22e9939deb485491fea82916d7b2b0","value":" 319/319 [10:52&lt;00:00,  1.90s/it]"}},"ef330dde15cd466cbe2919f8e04c5420":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff59be65d3e24c78b06c12859b258ef9","placeholder":"​","style":"IPY_MODEL_926ed50b87ea4347891156ccaec2504d","value":"Fetching 2 files: 100%"}},"f009210877e643198210edd4cbca3f6d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8d92f28397c44c88d3447d93d76c220":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f92b639211374dc8b17af498f9de646b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb3c107053574358924042cf41d29646":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd8d176344d9433f88a217562afc1695","IPY_MODEL_0f30865a130a4b4c80bf301244a2cbcd","IPY_MODEL_c76d5d0dcf8b4983a943f7a647c77a81"],"layout":"IPY_MODEL_9f9464bb8ac948b1a996653ce65c089f"}},"fbde23102e72480889184ec14f4dcf23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd8d176344d9433f88a217562afc1695":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72d70069a58b48749bbf309f1a8787a8","placeholder":"​","style":"IPY_MODEL_c1a3f1c56391476f86fb446cfcb8b165","value":"Loading weights: 100%"}},"ff59be65d3e24c78b06c12859b258ef9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14951862,"datasetId":9569465,"databundleVersionId":15822171}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BLIP-2 Explorer — LoRA Fine-Tuning on the Underwater Image Captioning Dataset (UICD)\n\n**Model:** [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)  \n**Dataset:** [kevintang2048/underwater-image-captioning-dataset-uicd](https://www.kaggle.com/datasets/kevintang2048/underwater-image-captioning-dataset-uicd)  \n**Accelerator:** Kaggle Notebooks — T4 x2\n\n---\n\n## Abstract\n\nThe application of large language models to image-to-text tasks in offline settings is often constrained by limited hardware resources, with deployment bottlenecks arising in embedded environments where high-accuracy models are too large for local operation. This limitation is particularly evident in marine imaging systems for ecological research, where real-time image captioning can enhance the utility of video footage by maintaining a searchable text-based record, yet has to operate entirely on-device in underwater environments with limited or unreliable internet connectivity. To address this challenge, we introduce **BLIP-2 Explorer**, a lightweight image captioning model based on BLIP-2 that is specialised for marine image captioning through domain-specific fine-tuning. In developing BLIP-2 Explorer, we combine knowledge distillation with low-rank adaptation (LoRA)-based parameter-efficient fine-tuning to substantially reduce the size of the original BLIP-2 architecture while preserving comparable image captioning performance in marine-specific contexts.\n\n---\n\n## Notebook Structure\n\n| # | Section |\n|---|---------|\n| 1 | Environment Setup |\n| 2 | GPU / Memory Check |\n| 3 | Configuration |\n| 4 | Dataset Loading |\n| 5 | Train / Val / Test Split |\n| 6 | PyTorch Dataset & DataLoaders |\n| 7 | Load Base Model & Processor |\n| 8 | Apply LoRA (PEFT) |\n| 9 | Optimizer & Scheduler |\n| 10 | Training Loop |\n| 11 | Loss Visualisation |\n| 12 | Generate Test Captions |\n| 13 | COCO Evaluation Metrics |\n| 14 | Save Outputs |","metadata":{}},{"cell_type":"markdown","source":"## 1 · Environment Setup","metadata":{}},{"cell_type":"code","source":"# ── Install packages not pre-installed in Kaggle Notebooks ──────────────────\nimport subprocess, sys\n\nsubprocess.run([\n    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n    \"peft>=0.5.0\",\n    \"pycocoevalcap\",\n    \"pycocotools\",\n    \"open_clip_torch\",\n], check=True)\n\n# ── Standard library ─────────────────────────────────────────────────────────\nimport gc\nimport json\nimport os\nimport random\nimport time\nfrom pathlib import Path\n\n# ── Third-party ───────────────────────────────────────────────────────────────\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\n# ── HuggingFace ───────────────────────────────────────────────────────────────\nfrom transformers import (\n    AutoProcessor,\n    Blip2ForConditionalGeneration,\n    get_linear_schedule_with_warmup,\n)\n\n# ── PEFT / LoRA ───────────────────────────────────────────────────────────────\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nprint(\"All imports successful.\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:56:35.320826Z","iopub.execute_input":"2026-02-27T06:56:35.321091Z","iopub.status.idle":"2026-02-27T06:57:02.453934Z","shell.execute_reply.started":"2026-02-27T06:56:35.321069Z","shell.execute_reply":"2026-02-27T06:57:02.453170Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2 · GPU / Memory Check","metadata":{}},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nif not torch.cuda.is_available():\n    raise EnvironmentError(\"No CUDA device found — enable the T4 x2 accelerator in Kaggle settings.\")\n\ndevice = torch.device(\"cuda\")\nn_gpus = torch.cuda.device_count()\nprint(f\"Number of GPUs: {n_gpus}\")\nfor i in range(n_gpus):\n    props = torch.cuda.get_device_properties(i)\n    total_vram = props.total_memory / 1024**3\n    free_vram  = (props.total_memory - torch.cuda.memory_allocated(i)) / 1024**3\n    print(f\"  GPU {i}: {props.name}  |  Total VRAM: {total_vram:.1f} GB  |  Free VRAM: {free_vram:.1f} GB\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:57:02.455459Z","iopub.execute_input":"2026-02-27T06:57:02.455991Z","iopub.status.idle":"2026-02-27T06:57:02.705482Z","shell.execute_reply.started":"2026-02-27T06:57:02.455963Z","shell.execute_reply":"2026-02-27T06:57:02.704760Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3 · Configuration","metadata":{}},{"cell_type":"code","source":"CONFIG = {\n    # ── Model ────────────────────────────────────────────────────────────────\n    \"model_id\": \"Salesforce/blip2-opt-2.7b\",\n\n    # ── Dataset ──────────────────────────────────────────────────────────────\n    \"dataset_root\": \"/kaggle/input/datasets/kevintang2048/underwater-image-captioning-dataset-uicd\",\n    \"split\": (0.8, 0.1, 0.1),   # train / val / test fractions\n    \"seed\": 42,\n\n    # ── Training ─────────────────────────────────────────────────────────────\n    \"epochs\": 5,\n    \"batch_size\": 3,\n    \"lr\": 2e-4,\n    \"max_length\": 256,           # tokenizer max sequence length\n    \"warmup_ratio\": 0.1,         # fraction of total steps used for lr warmup\n\n    # ── LoRA ─────────────────────────────────────────────────────────────────\n    \"lora_r\": 16,\n    \"lora_alpha\": 16,\n    \"lora_dropout\": 0.1,\n    \"lora_bias\": \"none\",          # \"all\" trains every model bias → fp16 overflow\n    \"lora_target_modules\": [\"q_proj\", \"k_proj\"],\n\n    # ── Output ───────────────────────────────────────────────────────────────\n    \"output_dir\": \"/kaggle/working/blip2_lora_uicd\",\n}\n\n# Create output directory\nPath(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n\n# Deterministic seeding\nrandom.seed(CONFIG[\"seed\"])\nnp.random.seed(CONFIG[\"seed\"])\ntorch.manual_seed(CONFIG[\"seed\"])\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(CONFIG[\"seed\"])\n\nprint(\"CONFIG:\")\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:57:02.706455Z","iopub.execute_input":"2026-02-27T06:57:02.706733Z","iopub.status.idle":"2026-02-27T06:57:02.718676Z","shell.execute_reply.started":"2026-02-27T06:57:02.706694Z","shell.execute_reply":"2026-02-27T06:57:02.718031Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4 · Dataset Loading\n\nParses `UIC-captions.txt` (format: `<image_id>#<caption_index> <caption_text>`) and pairs each image filename with its list of reference captions.","metadata":{}},{"cell_type":"code","source":"def load_captions(captions_path: Path) -> dict:\n    \"\"\"Parse UIC-captions.txt into {filename: [caption, ...]}.\"\"\"\n    captions_dict = {}\n    with open(captions_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Format: <img_id>#<n> <caption text>\n            parts = line.split(\" \", 1)\n            if len(parts) != 2:\n                continue\n            img_key, caption = parts\n            filename = img_key.split(\"#\")[0]\n            captions_dict.setdefault(filename, []).append(caption.strip())\n    return captions_dict\n\n\ndataset_root = Path(CONFIG[\"dataset_root\"])\n\n# Locate captions file and image directory\ncaptions_file = next(dataset_root.rglob(\"UIC-captions.txt\"), None)\nif captions_file is None:\n    raise FileNotFoundError(f\"UIC-captions.txt not found under {dataset_root}\")\n\ndataset_base = captions_file.parent\nimage_dir    = dataset_base / \"uic_224x224_image\"\nif not image_dir.exists():\n    # Fallback: look for any image-containing subdirectory\n    image_dirs = [d for d in dataset_base.iterdir() if d.is_dir()]\n    if not image_dirs:\n        raise FileNotFoundError(f\"No image directory found under {dataset_base}\")\n    image_dir = image_dirs[0]\n\ncaptions_dict = load_captions(captions_file)\n\n# Build flat dataset list\ndataset = []\nfor filename, captions in captions_dict.items():\n    img_path = image_dir / filename\n    if img_path.exists():\n        dataset.append({\n            \"image_path\":     str(img_path),\n            \"image_filename\": filename,\n            \"captions\":       captions,\n        })\n\nprint(f\"Captions file : {captions_file}\")\nprint(f\"Image directory: {image_dir}\")\nprint(f\"Total samples  : {len(dataset)}\")\nprint(f\"Sample entry   : {dataset[0]}\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:57:02.719661Z","iopub.execute_input":"2026-02-27T06:57:02.720085Z","iopub.status.idle":"2026-02-27T06:57:15.070187Z","shell.execute_reply.started":"2026-02-27T06:57:02.720048Z","shell.execute_reply":"2026-02-27T06:57:15.069497Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5 · Train / Val / Test Split","metadata":{}},{"cell_type":"code","source":"train_frac, val_frac, test_frac = CONFIG[\"split\"]\n\ndataset_sorted = sorted(dataset, key=lambda x: x[\"image_filename\"])\nrng = random.Random(CONFIG[\"seed\"])\nrng.shuffle(dataset_sorted)\n\nn = len(dataset_sorted)\nn_train = int(n * train_frac)\nn_val   = int(n * val_frac)\n\ntrain_set = dataset_sorted[:n_train]\nval_set   = dataset_sorted[n_train : n_train + n_val]\ntest_set  = dataset_sorted[n_train + n_val :]\n\nprint(f\"Total : {n}\")\nprint(f\"Train : {len(train_set)}  ({len(train_set)/n*100:.1f}%)\")\nprint(f\"Val   : {len(val_set)}   ({len(val_set)/n*100:.1f}%)\")\nprint(f\"Test  : {len(test_set)}  ({len(test_set)/n*100:.1f}%)\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:57:15.071847Z","iopub.execute_input":"2026-02-27T06:57:15.072179Z","iopub.status.idle":"2026-02-27T06:57:15.079381Z","shell.execute_reply.started":"2026-02-27T06:57:15.072155Z","shell.execute_reply":"2026-02-27T06:57:15.078795Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6 · PyTorch Dataset & DataLoaders\n\n`UICDataset` randomly selects one reference caption per image per training step, feeds it through the BLIP-2 processor, and masks padding tokens in `labels` to -100 so they are excluded from the cross-entropy loss.","metadata":{}},{"cell_type":"code","source":"class UICDataset(Dataset):\n    \"\"\"PyTorch Dataset for the Underwater Image Captioning Dataset.\"\"\"\n\n    def __init__(self, samples: list, processor, max_length: int = 256):\n        self.samples    = samples\n        self.processor  = processor\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        item = self.samples[idx]\n\n        # Load image\n        image = Image.open(item[\"image_path\"]).convert(\"RGB\")\n\n        # Randomly pick one reference caption for this step\n        caption = random.choice(item[\"captions\"])\n\n        # Tokenise\n        encoding = self.processor(\n            images=image,\n            text=caption,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n        )\n\n        # Remove batch dimension added by processor\n        pixel_values = encoding[\"pixel_values\"].squeeze(0)       # (C, H, W)\n        input_ids    = encoding[\"input_ids\"].squeeze(0)           # (L,)\n        attention_mask = encoding[\"attention_mask\"].squeeze(0)    # (L,)\n\n        # Labels: copy of input_ids with padding replaced by -100\n        labels = input_ids.clone()\n        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n\n        return {\n            \"pixel_values\":  pixel_values,\n            \"input_ids\":     input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\":        labels,\n        }\n\n\n# DataLoaders are created after the processor is loaded (next section).\n# They are defined here so the class is available for inspection.\nprint(\"UICDataset class defined.\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:57:15.080255Z","iopub.execute_input":"2026-02-27T06:57:15.080556Z","iopub.status.idle":"2026-02-27T06:57:15.505052Z","shell.execute_reply.started":"2026-02-27T06:57:15.080524Z","shell.execute_reply":"2026-02-27T06:57:15.504345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7 · Load Base Model & Processor\n\nLoads `Salesforce/blip2-opt-2.7b` in **fp16** with `device_map=\"auto\"` — Accelerate automatically pipelines the model across both T4 GPUs. All base parameters are frozen; only the LoRA adapters (added in the next section) will be trainable.","metadata":{}},{"cell_type":"code","source":"import warnings\n# Suppress harmless accelerate warning about query_tokens (nn.Parameter, not a submodule)\nwarnings.filterwarnings(\"ignore\", message=\".*query_tokens.*\")\n\ngc.collect()\ntorch.cuda.empty_cache()\n\nmodel_id = CONFIG[\"model_id\"]\n\n# ── Processor ─────────────────────────────────────────────────────────────────\nprint(\"Loading processor...\")\nprocessor = AutoProcessor.from_pretrained(model_id)\nprocessor.tokenizer.model_max_length = CONFIG[\"max_length\"]\n\n# ── Base model in fp16 — device_map=\"auto\" splits across both T4s ─────────────\nprint(\"Loading base model (fp16)...\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\n# Freeze all base parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nBase model loaded. Total parameters: {total_params / 1e9:.2f} B\")\nprint(f\"Device map: {model.hf_device_map if hasattr(model, 'hf_device_map') else 'single device'}\")\n\n\ndef _fix_language_model_device_map(m) -> None:\n    \"\"\"Ensure hf_device_map contains a 'language_model' entry.\n\n    When BLIP-2 is loaded with device_map='auto', Accelerate maps individual\n    sub-layers (e.g. 'language_model.model.decoder.layers.0') but omits the\n    top-level 'language_model' key.  BLIP-2's generate() checks for that key\n    and emits a spurious multi-GPU warning when it is absent.  We infer the\n    correct device from the first parameter of language_model and add the entry.\n    \"\"\"\n    base = getattr(m, \"base_model\", m)   # unwrap PEFT wrapper if present\n    hf_map = getattr(base, \"hf_device_map\", None)\n    if hf_map is None or \"language_model\" in hf_map:\n        return\n    lm = getattr(base, \"language_model\", None)\n    if lm is None:\n        return\n    try:\n        lm_device = next(lm.parameters()).device\n        hf_map[\"language_model\"] = lm_device.index if lm_device.type == \"cuda\" else lm_device\n        print(f\"hf_device_map patched: language_model → GPU {hf_map['language_model']}\")\n    except StopIteration:\n        pass\n\n\n_fix_language_model_device_map(model)\n\n# ── Build DataLoaders now that the processor is available ─────────────────────\n# num_workers=0 avoids multiprocessing AssertionError in Kaggle/Jupyter notebooks\ntrain_dataset = UICDataset(train_set, processor, CONFIG[\"max_length\"])\nval_dataset   = UICDataset(val_set,   processor, CONFIG[\"max_length\"])\ntest_dataset  = UICDataset(test_set,  processor, CONFIG[\"max_length\"])\n\ntrain_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True,  num_workers=0, pin_memory=True)\nval_loader   = DataLoader(val_dataset,   batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=0, pin_memory=True)\ntest_loader  = DataLoader(test_dataset,  batch_size=1,                    shuffle=False, num_workers=0, pin_memory=True)\n\nprint(f\"\\nTrain batches : {len(train_loader)}\")\nprint(f\"Val batches   : {len(val_loader)}\")\nprint(f\"Test batches  : {len(test_loader)}\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:57:15.506049Z","iopub.execute_input":"2026-02-27T06:57:15.506306Z","iopub.status.idle":"2026-02-27T06:58:18.536584Z","shell.execute_reply.started":"2026-02-27T06:57:15.506272Z","shell.execute_reply":"2026-02-27T06:58:18.535747Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8 · Apply LoRA (PEFT)\n\nInjects low-rank adapter matrices into the `q_proj` and `k_proj` layers of both the Q-Former cross-attention blocks and the OPT decoder — matching the \"query and key layers\" specification from the reference methodology. Only these adapter weights (~0.77% of total parameters) will be updated during training.","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=CONFIG[\"lora_r\"],\n    lora_alpha=CONFIG[\"lora_alpha\"],\n    lora_dropout=CONFIG[\"lora_dropout\"],\n    bias=CONFIG[\"lora_bias\"],\n    target_modules=CONFIG[\"lora_target_modules\"],\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Re-apply the device-map patch: get_peft_model() wraps the model in a new\n# PeftModel object whose hf_device_map may again be missing 'language_model'.\n_fix_language_model_device_map(model)\n\n# ── Trainable parameter report ────────────────────────────────────────────────\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nall_params        = sum(p.numel() for p in model.parameters())\npct               = 100 * trainable_params / all_params\n\nprint(f\"Trainable parameters : {trainable_params:,}  ({pct:.2f}% of total)\")\nprint(f\"Total parameters     : {all_params:,}\")\nmodel.print_trainable_parameters()\n","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:58:18.537560Z","iopub.execute_input":"2026-02-27T06:58:18.537816Z","iopub.status.idle":"2026-02-27T06:58:20.179567Z","shell.execute_reply.started":"2026-02-27T06:58:18.537791Z","shell.execute_reply":"2026-02-27T06:58:20.178832Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9 · Optimizer & Scheduler","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=CONFIG[\"lr\"],\n)\n\ntotal_training_steps = CONFIG[\"epochs\"] * len(train_loader)\nwarmup_steps = int(total_training_steps * CONFIG[\"warmup_ratio\"])\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_training_steps,\n)\n\nprint(f\"Total training steps : {total_training_steps}\")\nprint(f\"Warmup steps         : {warmup_steps}\")\nprint(f\"Learning rate        : {CONFIG['lr']}\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:58:20.181166Z","iopub.execute_input":"2026-02-27T06:58:20.181464Z","iopub.status.idle":"2026-02-27T06:58:27.033927Z","shell.execute_reply.started":"2026-02-27T06:58:20.181441Z","shell.execute_reply":"2026-02-27T06:58:27.033059Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10 · Training Loop\n\n- Per-step training loss is recorded at every gradient step for fine-grained visualisation.\n- Validation loss is computed at the end of each epoch (no generation — keeps epoch runtime manageable on T4 x2).\n- The best checkpoint (lowest validation loss) is saved to `output_dir/best_checkpoint/`.","metadata":{}},{"cell_type":"code","source":"# ── Tracking ──────────────────────────────────────────────────────────────────\nstep_train_losses = []   # list of (global_step, loss)\nepoch_val_losses  = []   # list of val_loss per epoch\nepoch_boundaries  = []   # global_step at the end of each epoch\nglobal_step       = 0\nbest_val_loss     = float(\"inf\")\n\nbest_ckpt_dir = Path(CONFIG[\"output_dir\"]) / \"best_checkpoint\"\n\n# TF32 uses fp32-range exponents in matmuls → avoids the +/-65504 fp16 overflow\n# that causes NaN in Q-Former attention from the very first forward pass on T4.\ntorch.backends.cuda.matmul.fp32_precision = \"tf32\"\ntorch.backends.cudnn.conv.fp32_precision  = \"tf32\"\n\n# GradScaler prevents fp16 gradient underflow/overflow that causes NaN losses.\n# Use a conservative init_scale (2^14 instead of default 2^16) to reduce early overflow.\nscaler = torch.amp.GradScaler(\"cuda\", init_scale=2**14)\n\nmodel.train()\n\nfor epoch in range(1, CONFIG[\"epochs\"] + 1):\n    # ── Training ──────────────────────────────────────────────────────────────\n    model.train()\n    train_loss_sum = 0.0\n    train_steps    = 0\n\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{CONFIG['epochs']} [train]\", leave=True)\n    for batch in pbar:\n        # Move tensors to the first CUDA device; model uses device_map=\"auto\"\n        batch = {k: v.to(device) for k, v in batch.items()}\n        # Explicitly cast pixel_values to fp16 to match model dtype and prevent\n        # implicit dtype promotion that can overflow fp16 range across devices.\n        if \"pixel_values\" in batch:\n            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(torch.float16)\n\n        optimizer.zero_grad()\n\n        with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n            outputs = model(**batch)\n            loss    = outputs.loss\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        loss_val = loss.item()\n\n        # Guard: skip NaN/inf steps (GradScaler already skips the optimizer step,\n        # but we must not accumulate NaN into the running sum)\n        if not torch.isfinite(torch.tensor(loss_val)):\n            global_step += 1\n            pbar.set_postfix({\"loss\": \"NaN – skipped\"})\n            continue\n\n        train_loss_sum += loss_val\n        train_steps    += 1\n        step_train_losses.append((global_step, loss_val))\n        global_step += 1\n\n        pbar.set_postfix({\"loss\": f\"{loss_val:.4f}\"})\n\n    avg_train_loss = train_loss_sum / max(train_steps, 1)\n    epoch_boundaries.append(global_step)\n\n    # ── Validation ────────────────────────────────────────────────────────────\n    model.eval()\n    val_loss_sum = 0.0\n    val_steps    = 0\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Epoch {epoch}/{CONFIG['epochs']} [val]\", leave=False):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            if \"pixel_values\" in batch:\n                batch[\"pixel_values\"] = batch[\"pixel_values\"].to(torch.float16)\n            with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n                outputs = model(**batch)\n            v_loss = outputs.loss.item()\n            if torch.isfinite(torch.tensor(v_loss)):\n                val_loss_sum += v_loss\n                val_steps    += 1\n\n    avg_val_loss = val_loss_sum / max(val_steps, 1)\n    epoch_val_losses.append(avg_val_loss)\n\n    print(f\"\\nEpoch {epoch}  |  train_loss: {avg_train_loss:.4f}  |  val_loss: {avg_val_loss:.4f}\")\n\n    # ── Save best checkpoint ──────────────────────────────────────────────────\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        model.save_pretrained(str(best_ckpt_dir))\n        processor.save_pretrained(str(best_ckpt_dir))\n        print(f\"  ✓ Best checkpoint saved (val_loss={best_val_loss:.4f})\")\n\nprint(\"\\nTraining complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:58:27.034952Z","iopub.execute_input":"2026-02-27T06:58:27.035250Z","iopub.status.idle":"2026-02-27T07:48:56.788993Z","shell.execute_reply.started":"2026-02-27T06:58:27.035224Z","shell.execute_reply":"2026-02-27T07:48:56.788233Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 11 · Loss Visualisation","metadata":{}},{"cell_type":"code","source":"def smooth(values: list, window: int = 50) -> np.ndarray:\n    \"\"\"Simple rolling-mean smoother (same-length output, edge-padded).\"\"\"\n    arr = np.array(values, dtype=np.float32)\n    if len(arr) < window:\n        return arr\n    kernel = np.ones(window) / window\n    return np.convolve(arr, kernel, mode=\"same\")\n\n\nsteps  = [s for s, _ in step_train_losses]\nlosses = [l for _, l in step_train_losses]\nsmoothed = smooth(losses)\n\nepochs_x = list(range(1, len(epoch_val_losses) + 1))\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\nfig.suptitle(\"BLIP-2 Explorer — Training Loss Curves\", fontsize=14, fontweight=\"bold\")\n\n# ── Left: per-step training loss ──────────────────────────────────────────────\nax1.plot(steps, losses,   color=\"steelblue\", alpha=0.25, linewidth=0.8, label=\"Per-step loss\")\nax1.plot(steps, smoothed, color=\"steelblue\", linewidth=1.8,              label=f\"Smoothed (window=50)\")\n\n# Epoch boundary markers\nfor i, boundary in enumerate(epoch_boundaries):\n    ax1.axvline(x=boundary, color=\"gray\", linestyle=\"--\", linewidth=0.8,\n                label=\"Epoch boundary\" if i == 0 else None)\n\nax1.set_xlabel(\"Training step\")\nax1.set_ylabel(\"Cross-entropy loss\")\nax1.set_title(\"Training Loss (per step)\")\nax1.legend(fontsize=9)\nax1.grid(True, alpha=0.3)\n\n# ── Right: per-epoch validation loss ─────────────────────────────────────────\nax2.plot(epochs_x, epoch_val_losses, color=\"darkorange\", marker=\"o\",\n         linewidth=1.8, markersize=6, label=\"Val loss\")\nax2.set_xlabel(\"Epoch\")\nax2.set_ylabel(\"Cross-entropy loss\")\nax2.set_title(\"Validation Loss (per epoch)\")\nax2.set_xticks(epochs_x)\nax2.legend(fontsize=9)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\n\nloss_plot_path = str(Path(CONFIG[\"output_dir\"]) / \"loss_curves.png\")\nplt.savefig(loss_plot_path, dpi=150, bbox_inches=\"tight\")\nprint(f\"Loss curves saved to: {loss_plot_path}\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T07:48:56.789934Z","iopub.execute_input":"2026-02-27T07:48:56.790227Z","iopub.status.idle":"2026-02-27T07:48:57.543282Z","shell.execute_reply.started":"2026-02-27T07:48:56.790198Z","shell.execute_reply":"2026-02-27T07:48:57.542536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 12 · Save Outputs\n\nSaves all artefacts to `/kaggle/working/blip2_lora_uicd/`:\n\n| Path | Contents |\n|------|----------|\n| `lora_adapters/` | LoRA adapter weights only (`adapter_model.bin` + config) |\n| `processor/` | Tokeniser and image processor |\n| `merged_model/` | Full merged model (base + LoRA fused, ~5 GB) |\n| `loss_curves.png` | Training / validation loss plots |","metadata":{}},{"cell_type":"code","source":"output_dir = Path(CONFIG[\"output_dir\"])\n\n# ── 1. LoRA adapter weights ───────────────────────────────────────────────────\nlora_dir = output_dir / \"lora_adapters\"\nmodel.save_pretrained(str(lora_dir))\nprint(f\"LoRA adapters saved  → {lora_dir}\")\n\n# ── 2. Processor ──────────────────────────────────────────────────────────────\nproc_dir = output_dir / \"processor\"\nprocessor.save_pretrained(str(proc_dir))\nprint(f\"Processor saved      → {proc_dir}\")\n\n# ── 3. Merged model (base + LoRA fused) ───────────────────────────────────────\nprint(\"Merging LoRA weights into base model (this may take a minute)...\")\nmerged_model = model.merge_and_unload()\nmerged_dir   = output_dir / \"merged_model\"\nmerged_model.save_pretrained(str(merged_dir))\nprint(f\"Merged model saved   → {merged_dir}\")\n\n# ── 4. Summary ────────────────────────────────────────────────────────────────\nprint(\"\\n── Saved files ──────────────────────────────────────────\")\nfor p in sorted(output_dir.rglob(\"*\")):\n    if p.is_file():\n        size_mb = p.stat().st_size / 1024**2\n        print(f\"  {p.relative_to(output_dir)}  ({size_mb:.1f} MB)\")\nprint(\"─────────────────────────────────────────────────────────\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T07:48:57.544304Z","iopub.execute_input":"2026-02-27T07:48:57.544606Z","iopub.status.idle":"2026-02-27T07:49:21.832929Z","shell.execute_reply.started":"2026-02-27T07:48:57.544576Z","shell.execute_reply":"2026-02-27T07:49:21.832220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 13 · Package Outputs as ZIP\n\nCreates `blip2_lora_uicd.zip` in `/kaggle/working/` containing all saved artefacts for easy download.","metadata":{}},{"cell_type":"code","source":"import zipfile\n\noutput_dir = Path(CONFIG[\"output_dir\"])\nzip_path   = output_dir.parent / f\"{output_dir.name}.zip\"\n\nprint(f\"Creating archive: {zip_path}\")\nwith zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n    for file in sorted(output_dir.rglob(\"*\")):\n        if file.is_file():\n            arcname = file.relative_to(output_dir.parent)\n            zf.write(file, arcname)\n\nzip_size_mb = zip_path.stat().st_size / 1024**2\nprint(f\"Done. Archive size: {zip_size_mb:.1f} MB  →  {zip_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T07:49:21.834388Z","iopub.execute_input":"2026-02-27T07:49:21.834634Z","iopub.status.idle":"2026-02-27T07:55:19.461798Z","shell.execute_reply.started":"2026-02-27T07:49:21.834609Z","shell.execute_reply":"2026-02-27T07:55:19.461019Z"}},"outputs":[],"execution_count":null}]}